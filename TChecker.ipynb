{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nadaaym/TChecker/blob/main/TChecker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmEMbOj8Zj27"
      },
      "outputs": [],
      "source": [
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.__version__\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plsclOnUZofx",
        "outputId": "ffc2f26c-d25d-4392-a77d-e191cede3ee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.19.0-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aPVLSXMcGjI"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-ignite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR90GGyobxLI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer,BertModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from argparse import ArgumentParser\n",
        "from ignite.engine import Engine,State, Events, create_supervised_trainer, create_supervised_evaluator\n",
        "from ignite.metrics import Accuracy, Loss\n",
        "from ignite.handlers import EarlyStopping\n",
        "from ignite.contrib.handlers import TensorboardLogger, ProgressBar\n",
        "from ignite.utils import convert_tensor\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qilf-HH8-ajd"
      },
      "outputs": [],
      "source": [
        "!pip3 install emoji==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ27tMOGfERH"
      },
      "outputs": [],
      "source": [
        "from transformers import BertweetTokenizer\n",
        "def Bert_Tokenizer(model_name):\n",
        "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "    tokenizer = BertweetTokenizer.from_pretrained(model_name)\n",
        "    return tokenizer\n",
        "tokenizer = Bert_Tokenizer('vinai/bertweet-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyhKlyzBZtZv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "dir=\"./drive/My Drive/Colab Notebooks/user_features/\"\n",
        "train=pd.read_csv(dir+\"gossip_train_users.csv\")   #sampled data, such that the two classes are balanced\n",
        "val=pd.read_csv(dir+\"gossip_val_users.csv\")\n",
        "test=pd.read_csv(dir+\"gossip_test_users.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOCmq9VKfLKL"
      },
      "outputs": [],
      "source": [
        "train['target']=train['target'].astype('category')\n",
        "train['target']=train['target'].cat.codes.astype('float')  #fake:0, real:1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m8GYb4onrS5"
      },
      "outputs": [],
      "source": [
        "val['target']=val['target'].astype('category')\n",
        "val['target']=val['target'].cat.codes.astype('float')  #fake:0, real:1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDzMG5Icnreg"
      },
      "outputs": [],
      "source": [
        "test['target']=test['target'].astype('category')\n",
        "test['target']=test['target'].cat.codes.astype('float')  #fake:0, real:1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmCos7HFgCbv"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self,df,tokenizer,twtokenizer,max_len,tw_len):\n",
        "\n",
        "        self.bert_encode = tokenizer\n",
        "        self.bertweet_encode=twtokenizer\n",
        "        self.texts = df.tweet_text.values\n",
        "        self.content=df.news_content.values\n",
        "        self.labels = df.target.values\n",
        "        self.max_len = max_len\n",
        "        self.tw_len=tw_len\n",
        "        self.df=df\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.texts)\n",
        "\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "\n",
        "        tw_tokens,tw_mask,tw_tokens_len = self.get_tw_mask(self.texts[idx],self.tw_len)\n",
        "        co_tokens,co_mask,co_tokens_len=self.get_token_mask(self.content[idx],self.max_len)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return [torch.tensor(tw_tokens),torch.tensor(tw_mask),torch.tensor(tw_tokens_len),torch.tensor(co_tokens),torch.tensor(co_mask),torch.tensor(co_tokens_len)],label\n",
        "\n",
        "    def get_token_mask(self,text,max_len):\n",
        "\n",
        "\n",
        "        tokens = []\n",
        "        mask = []\n",
        "        text = self.bert_encode.encode(text)\n",
        "        size = len(text)\n",
        "        if size < max_len:\n",
        "          pads = self.bert_encode.encode(['PAD']*(max(0,max_len-size)))\n",
        "\n",
        "          tokens[:max(max_len,size)] = text[:max(max_len,size)]\n",
        "          tokens = tokens + pads[1:-1]\n",
        "          mask = [1]*size+[0]*len(pads[1:-1])\n",
        "        elif size >= max_len:\n",
        "          tokens[:max_len] = text[:max_len]\n",
        "          mask = [1]*max_len\n",
        "\n",
        "        tokens_len = len(tokens)\n",
        "\n",
        "        return tokens,mask,tokens_len\n",
        "\n",
        "    def get_tw_mask(self,text,tw_len):\n",
        "\n",
        "        tokens = []\n",
        "        mask = []\n",
        "        text=re.sub(r'http\\S+', '', text)\n",
        "        text = self.bertweet_encode.encode(text)\n",
        "        size = len(text)\n",
        "        if size < tw_len:\n",
        "          pads = self.bertweet_encode.encode(['PAD']*(max(0,tw_len-size)))\n",
        "\n",
        "          tokens[:max(tw_len,size)] = text[:max(tw_len,size)]\n",
        "          tokens = tokens + pads[1:-1]\n",
        "          mask = [1]*size+[0]*len(pads[1:-1])\n",
        "        elif size >= tw_len:\n",
        "          tokens[:tw_len] = text[:tw_len]\n",
        "          mask = [1]*tw_len\n",
        "\n",
        "        tokens_len = len(tokens)\n",
        "\n",
        "\n",
        "        return tokens,mask,tokens_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jHKHO4ygKby"
      },
      "outputs": [],
      "source": [
        "def get_data_loaders():\n",
        "    train_dataset = TextDataset(train,tokenizer=tokenizer,twtokenizer=twtokenizer ,max_len=512,tw_len=128)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
        "    valid_dataset = TextDataset(val,tokenizer=tokenizer,twtokenizer=twtokenizer ,max_len=512,tw_len=128)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=16,shuffle=True)\n",
        "\n",
        "    return train_loader , valid_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z74RAui7gXSb"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, AutoModel\n",
        "\n",
        "\n",
        "class TChecker(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.twbert=AutoModel.from_pretrained('vinai/bertweet-base')\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.hidden_size_tw=self.twbert.config.hidden_size\n",
        "        self.LSTM = nn.LSTM(self.hidden_size,self.hidden_size,bidirectional=True) #self.hidden_size+7\n",
        "        self.twLSTM=nn.LSTM(self.hidden_size_tw,self.hidden_size_tw,bidirectional=True)\n",
        "        self.clf = nn.Linear((self.hidden_size*4),1)\n",
        "\n",
        "\n",
        "    def forward(self,inputs):\n",
        "\n",
        "        tw_encoded_layers, tw_pooled_output = self.twbert(input_ids=inputs[0],attention_mask=inputs[1],return_dict = False)\n",
        "        ti_encoded_layers,ti_pooled_output=self.bert(inputs[3],attention_mask=inputs[4],return_dict=False)\n",
        "\n",
        "        tw_encoded_layers = tw_encoded_layers.permute(1,0,2)\n",
        "        ti_encoded_layers=ti_encoded_layers.permute(1,0,2)\n",
        "        =\n",
        "        tw_enc_hiddens, (tw_last_hidden, tw_last_cell) = self.twLSTM(pack_padded_sequence(tw_encoded_layers,inputs[2].cpu()))\n",
        "        ti_enc_hiddens,(ti_last_hidden,ti_last_cell)=self.LSTM(pack_padded_sequence(ti_encoded_layers,inputs[5].cpu()))\n",
        "        output_hidden = torch.cat((tw_last_hidden[0], tw_last_hidden[1],ti_last_hidden[0],ti_last_hidden[1]), dim=1)\n",
        "        output_hidden = F.dropout(output_hidden,0.2)\n",
        "        output = self.clf(output_hidden)\n",
        "\n",
        "        return F.sigmoid(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1R6gJCMIga90"
      },
      "outputs": [],
      "source": [
        "def _prepare_batch(batch, device=None, non_blocking=False):\n",
        "\n",
        "    x, y = batch\n",
        "\n",
        "    return (convert_tensor(x, device=device, non_blocking=non_blocking),\n",
        "            convert_tensor(y, device=device, non_blocking=non_blocking))\n",
        "def create_supervised_trainer1(model, optimizer, loss_fn, metrics={}, device=None):\n",
        "\n",
        "    def _update(engine, batch):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        x, y = _prepare_batch(batch, device=device)\n",
        "        y_pred = model(x)\n",
        "        y=y.unsqueeze(1)\n",
        "        loss = loss_fn(y_pred, y.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        return loss.item(), y_pred, y\n",
        "\n",
        "    def _metrics_transform(output):\n",
        "        return output[1], output[2]\n",
        "\n",
        "    engine = Engine(_update)\n",
        "\n",
        "    for name, metric in metrics.items():\n",
        "        metric._output_transform = _metrics_transform\n",
        "        metric.attach(engine, name)\n",
        "\n",
        "    return engine\n",
        "\n",
        "def create_supervised_evaluator1(model, metrics=None,\n",
        "                                device=None, non_blocking=False,\n",
        "                                prepare_batch=_prepare_batch,\n",
        "                                output_transform=lambda x, y, y_pred: (y_pred, y,)):\n",
        "\n",
        "    metrics = metrics or {}\n",
        "\n",
        "    if device:\n",
        "        model\n",
        "\n",
        "    def _inference(engine, batch):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            x, y = prepare_batch(batch, device=device, non_blocking=non_blocking)\n",
        "            y_pred = model(x)\n",
        "            y=y.unsqueeze(1)\n",
        "            return output_transform(x, y.float(), y_pred)\n",
        "\n",
        "    engine = Engine(_inference)\n",
        "\n",
        "    for name, metric in metrics.items():\n",
        "        metric.attach(engine, name)\n",
        "\n",
        "    return engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crkiYvW3glMQ"
      },
      "outputs": [],
      "source": [
        "def run(log_interval=100,epochs=4,lr=0.000006):\n",
        "    train_loader ,valid_loader = get_data_loaders()\n",
        "    model = TChecker()\n",
        "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    lr_scheduler = ExponentialLR(optimizer, gamma=0.90)\n",
        "    trainer = create_supervised_trainer1(model.to(device), optimizer, criterion, device=device)\n",
        "    evaluator = create_supervised_evaluator1(model.to(device), metrics={'BCELoss': Loss(criterion)}, device=device)\n",
        "\n",
        "    if log_interval is None:\n",
        "        e = Events.ITERATION_COMPLETED\n",
        "        log_interval = 1\n",
        "    else:\n",
        "        e = Events.ITERATION_COMPLETED(every=log_interval)\n",
        "\n",
        "    desc = \"loss: {:.4f} | lr: {:.4f}\"\n",
        "    pbar = tqdm(\n",
        "        initial=0, leave=False, total=len(train_loader),\n",
        "        desc=desc.format(0, lr)\n",
        "    )\n",
        "    @trainer.on(e)\n",
        "    def log_training_loss(engine):\n",
        "        pbar.refresh()\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "        pbar.desc = desc.format(engine.state.output[0], lr)\n",
        "        pbar.update(log_interval)\n",
        "\n",
        "    @trainer.on(Events.EPOCH_COMPLETED)\n",
        "    def update_lr_scheduler(engine):\n",
        "        lr_scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "    @trainer.on(Events.EPOCH_COMPLETED)\n",
        "    def log_training_results(engine):\n",
        "        evaluator.run(train_loader)\n",
        "        metrics = evaluator.state.metrics\n",
        "        avg_loss = metrics['BCELoss']\n",
        "        tqdm.write(\n",
        "            \"Train Epoch: {} BCE loss: {:.2f}\".format(engine.state.epoch, avg_loss)\n",
        "        )\n",
        "\n",
        "    @trainer.on(Events.EPOCH_COMPLETED)\n",
        "    def log_validation_results(engine):\n",
        "        pbar.refresh()\n",
        "        evaluator.run(valid_loader)\n",
        "        metrics = evaluator.state.metrics\n",
        "        avg_loss = metrics['BCELoss']\n",
        "        tqdm.write(\n",
        "            \"Valid Epoch: {} BCE loss: {:.2f}\".format(engine.state.epoch, avg_loss)\n",
        "        )\n",
        "        pbar.n = pbar.last_print_n = 0\n",
        "\n",
        "\n",
        "    try:\n",
        "        trainer.run(train_loader, max_epochs=epochs)\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doelOGeZgsh2"
      },
      "outputs": [],
      "source": [
        "model =run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l65-ZbbMBjLL"
      },
      "outputs": [],
      "source": [
        "class TestTextDataset(Dataset):\n",
        "    def __init__(self,df,tokenizer,max_len):\n",
        "\n",
        "        self.bert_encode = tokenizer\n",
        "        self.texts = df.tweet_text.values\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "\n",
        "        tokens,mask,tokens_len = self.get_token_mask(self.texts[idx],self.max_len)\n",
        "        return [torch.tensor(tokens),torch.tensor(mask),torch.tensor(tokens_len)]\n",
        "\n",
        "    def get_token_mask(self,text,max_len):\n",
        "\n",
        "        tokens = []\n",
        "        mask = []\n",
        "        text = self.bert_encode.encode(text)\n",
        "        size = len(text)\n",
        "        if size < max_len:\n",
        "          pads = self.bert_encode.encode(['PAD']*(max(0,max_len-size)))\n",
        "          #print(\"padded\")\n",
        "          tokens[:max(max_len,size)] = text[:max(max_len,size)]\n",
        "          tokens = tokens + pads[1:-1]\n",
        "          mask = [1]*size+[0]*len(pads[1:-1])\n",
        "        elif size >= max_len:\n",
        "          tokens[:max_len] = text[:max_len]\n",
        "          mask = [1]*max_len\n",
        "\n",
        "        tokens_len = len(tokens)\n",
        "\n",
        "        return tokens,mask,tokens_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkeCxBEJBpr_"
      },
      "outputs": [],
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "model.eval()\n",
        "predictions = []\n",
        "test_dataset = TestTextDataset(test,tokenizer=tokenizer,max_len=128)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=32,shuffle=False)\n",
        "with torch.no_grad():\n",
        "    for idx , (inputs) in tqdm(enumerate(test_loader),total=len(test_loader)):\n",
        "        inputs = [a.to(device) for a in inputs]\n",
        "        preds = model(inputs)\n",
        "        predictions.append(preds.cpu().detach().numpy())\n",
        "\n",
        "predictions = np.vstack(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RhXzc_pDGuk"
      },
      "outputs": [],
      "source": [
        "predictions=np.round(predictions).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEfwJfttB6xL"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "print(classification_report(test.target,predictions))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPLO+eCcVqjC5+iWXOdPeTa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}